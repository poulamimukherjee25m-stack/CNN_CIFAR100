{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GndH_CE_7khK",
        "outputId": "7948cbb2-99dc-48f8-f1eb-486d33c6265e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (26.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cpu)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CNN for CIFAR-100 — thesis-oriented baseline.\n",
        "Target: ~60–65% test accuracy without transfer learning.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchmetrics"
      ],
      "metadata": {
        "id": "5GnY8VXZ7ZWF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Data — CIFAR-100 correct normalization + augmentation (train only)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Per-channel normalization for CIFAR-100 (mandatory)\n",
        "CIFAR100_MEAN = (0.5071, 0.4867, 0.4408)\n",
        "CIFAR100_STD = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),   # 32→36 padded then crop 32\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD),\n",
        "])\n",
        "\n",
        "batch_size = 128\n",
        "print(\"Loading CIFAR-100 dataset...\")\n",
        "train_dataset = datasets.CIFAR100(root=\"dataset/\", download=True, train=True, transform=transform_train)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR100(root=\"dataset/\", download=True, train=False, transform=transform_test)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}, Test samples: {len(test_dataset)}, Classes: 100\")\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    \"\"\"Display image tensor (denormalized for visualization).\"\"\"\n",
        "    img = img * torch.tensor(CIFAR100_STD).view(3, 1, 1) + torch.tensor(CIFAR100_MEAN).view(3, 1, 1)\n",
        "    img = torch.clamp(img, 0, 1)\n",
        "    plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIa-5duT7yrG",
        "outputId": "c7e646f6-e969-4782-b56c-3b5a8069ac38"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CIFAR-100 dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:09<00:00, 18.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 50000, Test samples: 10000, Classes: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# CNN: 8 conv layers in 4 blocks — [Conv→BatchNorm→ReLU]×2 → MaxPool → Dropout\n",
        "# Block 1: 32 filters, Block 2: 64, Block 3: 128, Block 4: 256 → GAP → Dense(100)\n",
        "# ---------------------------------------------------------------------------\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1: 2 Conv (32 filters)\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.25),\n",
        "            # Block 2: 2 Conv (64 filters)\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.25),\n",
        "            # Block 3: 2 Conv (128 filters)\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.3),\n",
        "            # Block 4: 2 Conv (256 filters)\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "sZp1qJla7-24"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Training: SGD + momentum, weight decay, cosine LR, 150–200 epochs\n",
        "# ---------------------------------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = CNN(num_classes=100).to(device)\n",
        "print(\"\\nModel architecture:\")\n",
        "print(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "num_epochs = 50\n",
        "\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "print(f\"\\nStarting training for {num_epochs} epochs (SGD lr=0.1, momentum=0.9, weight_decay=1e-4, CosineAnnealing)...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    for data, targets in pbar:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "    scheduler.step()\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] avg_loss: {avg_loss:.4f} lr: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "print(\"Training completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcTn2vgP8DPN",
        "outputId": "aa7e263a-fcd6-4413-a9f7-f3e523589f8b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "\n",
            "Model architecture:\n",
            "CNN(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Dropout2d(p=0.25, inplace=False)\n",
            "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (15): Dropout2d(p=0.25, inplace=False)\n",
            "    (16): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (21): ReLU(inplace=True)\n",
            "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (23): Dropout2d(p=0.3, inplace=False)\n",
            "    (24): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): AdaptiveAvgPool2d(output_size=1)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=256, out_features=100, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Starting training for 50 epochs (SGD lr=0.1, momentum=0.9, weight_decay=1e-4, CosineAnnealing)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [1/50]: 100%|██████████| 391/391 [07:39<00:00,  1.18s/it, loss=4.1658]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50] avg_loss: 4.2083 lr: 0.099901\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [2/50]: 100%|██████████| 391/391 [07:36<00:00,  1.17s/it, loss=4.0898]\n",
            "Epoch [3/50]: 100%|██████████| 391/391 [07:39<00:00,  1.17s/it, loss=3.7445]\n",
            "Epoch [4/50]: 100%|██████████| 391/391 [07:38<00:00,  1.17s/it, loss=3.7833]\n",
            "Epoch [5/50]: 100%|██████████| 391/391 [07:38<00:00,  1.17s/it, loss=3.4623]\n",
            "Epoch [6/50]: 100%|██████████| 391/391 [07:37<00:00,  1.17s/it, loss=3.3107]\n",
            "Epoch [7/50]: 100%|██████████| 391/391 [07:38<00:00,  1.17s/it, loss=3.4019]\n",
            "Epoch [8/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=3.3312]\n",
            "Epoch [9/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=2.9255]\n",
            "Epoch [10/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=2.5893]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/50] avg_loss: 2.9940 lr: 0.090451\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [11/50]: 100%|██████████| 391/391 [07:32<00:00,  1.16s/it, loss=2.9502]\n",
            "Epoch [12/50]: 100%|██████████| 391/391 [07:35<00:00,  1.16s/it, loss=2.7282]\n",
            "Epoch [13/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=2.4766]\n",
            "Epoch [14/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=2.8138]\n",
            "Epoch [15/50]: 100%|██████████| 391/391 [07:32<00:00,  1.16s/it, loss=2.8092]\n",
            "Epoch [16/50]: 100%|██████████| 391/391 [07:29<00:00,  1.15s/it, loss=2.6757]\n",
            "Epoch [17/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=2.5757]\n",
            "Epoch [18/50]: 100%|██████████| 391/391 [07:32<00:00,  1.16s/it, loss=2.2418]\n",
            "Epoch [19/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=2.6874]\n",
            "Epoch [20/50]: 100%|██████████| 391/391 [07:31<00:00,  1.16s/it, loss=2.6734]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20/50] avg_loss: 2.4218 lr: 0.065451\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [21/50]: 100%|██████████| 391/391 [07:34<00:00,  1.16s/it, loss=2.5013]\n",
            "Epoch [22/50]: 100%|██████████| 391/391 [07:31<00:00,  1.15s/it, loss=2.7316]\n",
            "Epoch [23/50]: 100%|██████████| 391/391 [07:31<00:00,  1.16s/it, loss=2.4735]\n",
            "Epoch [24/50]: 100%|██████████| 391/391 [07:32<00:00,  1.16s/it, loss=2.3298]\n",
            "Epoch [25/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=2.2909]\n",
            "Epoch [26/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=2.1809]\n",
            "Epoch [27/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=1.8683]\n",
            "Epoch [28/50]: 100%|██████████| 391/391 [07:34<00:00,  1.16s/it, loss=2.3088]\n",
            "Epoch [29/50]: 100%|██████████| 391/391 [07:32<00:00,  1.16s/it, loss=2.3084]\n",
            "Epoch [30/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=2.2064]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [30/50] avg_loss: 2.1127 lr: 0.034549\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [31/50]: 100%|██████████| 391/391 [07:32<00:00,  1.16s/it, loss=1.9035]\n",
            "Epoch [32/50]: 100%|██████████| 391/391 [07:32<00:00,  1.16s/it, loss=2.1813]\n",
            "Epoch [33/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=2.1495]\n",
            "Epoch [34/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=1.8281]\n",
            "Epoch [35/50]: 100%|██████████| 391/391 [07:34<00:00,  1.16s/it, loss=2.3586]\n",
            "Epoch [36/50]: 100%|██████████| 391/391 [07:32<00:00,  1.16s/it, loss=1.9484]\n",
            "Epoch [37/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=1.7314]\n",
            "Epoch [38/50]: 100%|██████████| 391/391 [07:34<00:00,  1.16s/it, loss=1.8439]\n",
            "Epoch [39/50]: 100%|██████████| 391/391 [07:34<00:00,  1.16s/it, loss=1.8786]\n",
            "Epoch [40/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=1.7326]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [40/50] avg_loss: 1.8671 lr: 0.009549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [41/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=1.6804]\n",
            "Epoch [42/50]: 100%|██████████| 391/391 [07:34<00:00,  1.16s/it, loss=1.8373]\n",
            "Epoch [43/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=1.7482]\n",
            "Epoch [44/50]: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it, loss=1.5029]\n",
            "Epoch [45/50]: 100%|██████████| 391/391 [07:36<00:00,  1.17s/it, loss=1.4874]\n",
            "Epoch [46/50]: 100%|██████████| 391/391 [07:34<00:00,  1.16s/it, loss=2.0616]\n",
            "Epoch [47/50]: 100%|██████████| 391/391 [07:35<00:00,  1.17s/it, loss=1.9479]\n",
            "Epoch [48/50]: 100%|██████████| 391/391 [07:39<00:00,  1.18s/it, loss=1.9631]\n",
            "Epoch [49/50]: 100%|██████████| 391/391 [07:36<00:00,  1.17s/it, loss=1.7691]\n",
            "Epoch [50/50]: 100%|██████████| 391/391 [07:38<00:00,  1.17s/it, loss=2.0763]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/50] avg_loss: 1.7731 lr: 0.000000\n",
            "Training completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOQZwxBG7MU_",
        "outputId": "adf0c92b-9095-4958-ee21-ffb9e94cce45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 79/79 [00:37<00:00,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Results:\n",
            "  Test Accuracy:  0.5752 (57.52%)\n",
            "  Test Precision: 0.5849\n",
            "  Test Recall:    0.5752\n",
            "\n",
            "============================================================\n",
            "CNN Training and Evaluation Complete!\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Evaluation\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "acc_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=100)\n",
        "precision_metric = torchmetrics.Precision(task=\"multiclass\", num_classes=100, average=\"macro\")\n",
        "recall_metric = torchmetrics.Recall(task=\"multiclass\", num_classes=100, average=\"macro\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        acc_metric(preds.cpu(), labels.cpu())\n",
        "        precision_metric(preds.cpu(), labels.cpu())\n",
        "        recall_metric(preds.cpu(), labels.cpu())\n",
        "\n",
        "test_accuracy = acc_metric.compute()\n",
        "test_precision = precision_metric.compute()\n",
        "test_recall = recall_metric.compute()\n",
        "\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"  Test Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"  Test Precision: {test_precision:.4f}\")\n",
        "print(f\"  Test Recall:    {test_recall:.4f}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CNN Training and Evaluation Complete!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ]
}