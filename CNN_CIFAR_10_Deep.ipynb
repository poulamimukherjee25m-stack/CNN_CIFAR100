{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emuo7vTBVMKR"
      },
      "source": [
        "CNN_CIFAR10 simple CNN with just 3convolution layer for an accuracy of 75%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ftlsjsM4VGR_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --------------------\n",
        "# Device\n",
        "# --------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --------------------\n",
        "# Hyperparameters\n",
        "# --------------------\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "# --------------------\n",
        "# Data transforms\n",
        "# --------------------\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "# --------------------\n",
        "# Dataset & Loader\n",
        "# --------------------\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform_train\n",
        ")\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform_test\n",
        ")\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# --------------------\n",
        "# Simple CNN Model\n",
        "# --------------------\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc = nn.Linear(128 * 4 * 4, 10)  # 32x32 → 16 → 8 → 4 after pooling\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "# --------------------\n",
        "# Loss & Optimizer\n",
        "# --------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# --------------------\n",
        "# Training function\n",
        "# --------------------\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    print(f\"Epoch [{epoch}] Train Acc: {acc:.2f}%\")\n",
        "\n",
        "# --------------------\n",
        "# Testing function\n",
        "# --------------------\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    print(f\"Epoch [{epoch}] Test Acc: {acc:.2f}%\")\n",
        "    return acc\n",
        "\n",
        "# --------------------\n",
        "# Main loop\n",
        "# --------------------\n",
        "best_acc = 0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    acc = test(epoch)\n",
        "    best_acc = max(best_acc, acc)\n",
        "\n",
        "print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fMapHffz-0o"
      },
      "source": [
        "CIFAR-10 with deeper convolution layer and Batch Normalization\n",
        "\n",
        "Dropout for regularization\n",
        "\n",
        "Stronger data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YosQwvox0F14",
        "outputId": "c102c2b5-c721-4920-a910-678b61c51b5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 44.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1] Batch [0/391] Loss: 2.469\n",
            "Epoch [1] Batch [50/391] Loss: 1.649\n",
            "Epoch [1] Batch [100/391] Loss: 1.578\n",
            "Epoch [1] Batch [150/391] Loss: 1.502\n",
            "Epoch [1] Batch [200/391] Loss: 1.331\n",
            "Epoch [1] Batch [250/391] Loss: 1.179\n",
            "Epoch [1] Batch [300/391] Loss: 1.142\n",
            "Epoch [1] Batch [350/391] Loss: 0.934\n",
            "Epoch [1] Train Accuracy: 50.18%\n",
            "Epoch [1] Test Accuracy: 64.92%\n",
            "Epoch [2] Batch [0/391] Loss: 0.841\n",
            "Epoch [2] Batch [50/391] Loss: 1.000\n",
            "Epoch [2] Batch [100/391] Loss: 1.122\n",
            "Epoch [2] Batch [150/391] Loss: 0.974\n",
            "Epoch [2] Batch [200/391] Loss: 0.829\n",
            "Epoch [2] Batch [250/391] Loss: 0.941\n",
            "Epoch [2] Batch [300/391] Loss: 0.849\n",
            "Epoch [2] Batch [350/391] Loss: 0.928\n",
            "Epoch [2] Train Accuracy: 67.12%\n",
            "Epoch [2] Test Accuracy: 66.69%\n",
            "Epoch [3] Batch [0/391] Loss: 0.828\n",
            "Epoch [3] Batch [50/391] Loss: 0.737\n",
            "Epoch [3] Batch [100/391] Loss: 0.698\n",
            "Epoch [3] Batch [150/391] Loss: 0.724\n",
            "Epoch [3] Batch [200/391] Loss: 0.661\n",
            "Epoch [3] Batch [250/391] Loss: 0.884\n",
            "Epoch [3] Batch [300/391] Loss: 0.755\n",
            "Epoch [3] Batch [350/391] Loss: 0.784\n",
            "Epoch [3] Train Accuracy: 72.55%\n",
            "Epoch [3] Test Accuracy: 75.68%\n",
            "Epoch [4] Batch [0/391] Loss: 0.703\n",
            "Epoch [4] Batch [50/391] Loss: 0.810\n",
            "Epoch [4] Batch [100/391] Loss: 0.929\n",
            "Epoch [4] Batch [150/391] Loss: 0.667\n",
            "Epoch [4] Batch [200/391] Loss: 0.740\n",
            "Epoch [4] Batch [250/391] Loss: 0.718\n",
            "Epoch [4] Batch [300/391] Loss: 0.461\n",
            "Epoch [4] Batch [350/391] Loss: 0.537\n",
            "Epoch [4] Train Accuracy: 75.77%\n",
            "Epoch [4] Test Accuracy: 79.84%\n",
            "Epoch [5] Batch [0/391] Loss: 0.598\n",
            "Epoch [5] Batch [50/391] Loss: 0.600\n",
            "Epoch [5] Batch [100/391] Loss: 0.483\n",
            "Epoch [5] Batch [150/391] Loss: 0.674\n",
            "Epoch [5] Batch [200/391] Loss: 0.660\n",
            "Epoch [5] Batch [250/391] Loss: 0.696\n",
            "Epoch [5] Batch [300/391] Loss: 0.584\n",
            "Epoch [5] Batch [350/391] Loss: 0.675\n",
            "Epoch [5] Train Accuracy: 78.08%\n",
            "Epoch [5] Test Accuracy: 78.98%\n",
            "Epoch [6] Batch [0/391] Loss: 0.677\n",
            "Epoch [6] Batch [50/391] Loss: 0.666\n",
            "Epoch [6] Batch [100/391] Loss: 0.634\n",
            "Epoch [6] Batch [150/391] Loss: 0.518\n",
            "Epoch [6] Batch [200/391] Loss: 0.686\n",
            "Epoch [6] Batch [250/391] Loss: 0.715\n",
            "Epoch [6] Batch [300/391] Loss: 0.543\n",
            "Epoch [6] Batch [350/391] Loss: 0.697\n",
            "Epoch [6] Train Accuracy: 79.87%\n",
            "Epoch [6] Test Accuracy: 81.29%\n",
            "Epoch [7] Batch [0/391] Loss: 0.535\n",
            "Epoch [7] Batch [50/391] Loss: 0.761\n",
            "Epoch [7] Batch [100/391] Loss: 0.628\n",
            "Epoch [7] Batch [150/391] Loss: 0.601\n",
            "Epoch [7] Batch [200/391] Loss: 0.618\n",
            "Epoch [7] Batch [250/391] Loss: 0.562\n",
            "Epoch [7] Batch [300/391] Loss: 0.568\n",
            "Epoch [7] Batch [350/391] Loss: 0.324\n",
            "Epoch [7] Train Accuracy: 81.09%\n",
            "Epoch [7] Test Accuracy: 82.00%\n",
            "Epoch [8] Batch [0/391] Loss: 0.446\n",
            "Epoch [8] Batch [50/391] Loss: 0.553\n",
            "Epoch [8] Batch [100/391] Loss: 0.805\n",
            "Epoch [8] Batch [150/391] Loss: 0.515\n",
            "Epoch [8] Batch [200/391] Loss: 0.602\n",
            "Epoch [8] Batch [250/391] Loss: 0.378\n",
            "Epoch [8] Batch [300/391] Loss: 0.433\n",
            "Epoch [8] Batch [350/391] Loss: 0.562\n",
            "Epoch [8] Train Accuracy: 82.26%\n",
            "Epoch [8] Test Accuracy: 84.86%\n",
            "Epoch [9] Batch [0/391] Loss: 0.674\n",
            "Epoch [9] Batch [50/391] Loss: 0.501\n",
            "Epoch [9] Batch [100/391] Loss: 0.507\n",
            "Epoch [9] Batch [150/391] Loss: 0.534\n",
            "Epoch [9] Batch [200/391] Loss: 0.421\n",
            "Epoch [9] Batch [250/391] Loss: 0.423\n",
            "Epoch [9] Batch [300/391] Loss: 0.440\n",
            "Epoch [9] Batch [350/391] Loss: 0.463\n",
            "Epoch [9] Train Accuracy: 83.22%\n",
            "Epoch [9] Test Accuracy: 85.33%\n",
            "Epoch [10] Batch [0/391] Loss: 0.451\n",
            "Epoch [10] Batch [50/391] Loss: 0.366\n",
            "Epoch [10] Batch [100/391] Loss: 0.504\n",
            "Epoch [10] Batch [150/391] Loss: 0.496\n",
            "Epoch [10] Batch [200/391] Loss: 0.460\n",
            "Epoch [10] Batch [250/391] Loss: 0.386\n",
            "Epoch [10] Batch [300/391] Loss: 0.424\n",
            "Epoch [10] Batch [350/391] Loss: 0.485\n",
            "Epoch [10] Train Accuracy: 84.27%\n",
            "Epoch [10] Test Accuracy: 83.83%\n",
            "Epoch [11] Batch [0/391] Loss: 0.416\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --------------------\n",
        "# Device\n",
        "# --------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --------------------\n",
        "# Hyperparameters\n",
        "# --------------------\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# --------------------\n",
        "# Data transforms (stronger augmentation)\n",
        "# --------------------\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# --------------------\n",
        "# Deeper CNN Model\n",
        "# --------------------\n",
        "class DeepCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepCNN, self).__init__()\n",
        "        # Conv Block 1\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool1 = nn.MaxPool2d(2,2)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "\n",
        "        # Conv Block 2\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.pool2 = nn.MaxPool2d(2,2)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "\n",
        "        # Conv Block 3\n",
        "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(256)\n",
        "        self.pool3 = nn.MaxPool2d(2,2)\n",
        "        self.dropout3 = nn.Dropout(0.25)\n",
        "\n",
        "        # Fully connected\n",
        "        self.fc1 = nn.Linear(256*4*4, 512)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
        "        self.dropout_fc1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.relu(self.bn5(self.conv5(x)))\n",
        "        x = self.relu(self.bn6(self.conv6(x)))\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.bn_fc1(self.fc1(x)))\n",
        "        x = self.dropout_fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = DeepCNN().to(device)\n",
        "\n",
        "# --------------------\n",
        "# Loss & Optimizer\n",
        "# --------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# --------------------\n",
        "# Training function\n",
        "# --------------------\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        # Print every 50 batches\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"Epoch [{epoch}] Batch [{batch_idx}/{len(trainloader)}] Loss: {loss.item():.3f}\")\n",
        "\n",
        "    acc = 100.*correct/total\n",
        "    print(f\"Epoch [{epoch}] Train Accuracy: {acc:.2f}%\")\n",
        "\n",
        "# --------------------\n",
        "# Testing function\n",
        "# --------------------\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100.*correct/total\n",
        "    print(f\"Epoch [{epoch}] Test Accuracy: {acc:.2f}%\")\n",
        "    return acc\n",
        "\n",
        "# --------------------\n",
        "# Main Loop\n",
        "# --------------------\n",
        "best_acc = 0.0\n",
        "for epoch in range(1, epochs+1):\n",
        "    train(epoch)\n",
        "    acc = test(epoch)\n",
        "    best_acc = max(best_acc, acc)\n",
        "\n",
        "print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}